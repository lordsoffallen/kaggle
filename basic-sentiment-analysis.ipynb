{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2415872,"sourceType":"datasetVersion","datasetId":1461623}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-08T13:21:29.885570Z","iopub.execute_input":"2024-01-08T13:21:29.886444Z","iopub.status.idle":"2024-01-08T13:21:30.226439Z","shell.execute_reply.started":"2024-01-08T13:21:29.886409Z","shell.execute_reply":"2024-01-08T13:21:30.225612Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/amazon-product-reviews/Reviews.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/amazon-product-reviews/Reviews.csv')\ndf.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2024-01-08T13:21:30.314033Z","iopub.execute_input":"2024-01-08T13:21:30.314411Z","iopub.status.idle":"2024-01-08T13:21:37.864321Z","shell.execute_reply.started":"2024-01-08T13:21:30.314385Z","shell.execute_reply":"2024-01-08T13:21:37.863339Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 568454 entries, 0 to 568453\nData columns (total 10 columns):\n #   Column                  Non-Null Count   Dtype \n---  ------                  --------------   ----- \n 0   Id                      568454 non-null  int64 \n 1   ProductId               568454 non-null  object\n 2   UserId                  568454 non-null  object\n 3   ProfileName             568428 non-null  object\n 4   HelpfulnessNumerator    568454 non-null  int64 \n 5   HelpfulnessDenominator  568454 non-null  int64 \n 6   Score                   568454 non-null  int64 \n 7   Time                    568454 non-null  int64 \n 8   Summary                 568427 non-null  object\n 9   Text                    568454 non-null  object\ndtypes: int64(5), object(5)\nmemory usage: 445.8 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"df.Score.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T13:21:37.865878Z","iopub.execute_input":"2024-01-08T13:21:37.866195Z","iopub.status.idle":"2024-01-08T13:21:37.882964Z","shell.execute_reply.started":"2024-01-08T13:21:37.866170Z","shell.execute_reply":"2024-01-08T13:21:37.882215Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Score\n5    0.638789\n4    0.141885\n1    0.091948\n3    0.075010\n2    0.052368\nName: proportion, dtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"Score 4 & 5 is positive, score 1 & 2 is negative and 3 neutral. Let's stick to binary case but also resample to balance our data distrubution as ~77% of our current is data positive sentiment.","metadata":{}},{"cell_type":"code","source":"ds = df.groupby('Score').apply(lambda x: x.sample(min(50_000, len(x)), random_state=42)).reset_index(drop=True)\nds.Score.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T13:21:37.884085Z","iopub.execute_input":"2024-01-08T13:21:37.884352Z","iopub.status.idle":"2024-01-08T13:21:38.322179Z","shell.execute_reply.started":"2024-01-08T13:21:37.884330Z","shell.execute_reply":"2024-01-08T13:21:38.320694Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Score\n1    0.224811\n4    0.224811\n5    0.224811\n3    0.191719\n2    0.133848\nName: proportion, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"ds = ds[['Id', 'ProductId', 'Summary', 'Text', 'Score']]\nds.rename(columns={\n    \"Id\": \"id\",\n    \"ProductId\": \"product_id\",\n    \"Summary\": \"summary\",\n    \"Text\": \"text\",\n    \"Score\": \"score\"\n}, inplace=True)\n\nds['sentiment'] = np.where(ds['score'] > 3, 1, np.where(ds['score'] < 3, 0 , np.nan))\nds = ds[~(ds['score'] == 3)]\nds.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-08T13:21:38.324023Z","iopub.execute_input":"2024-01-08T13:21:38.324348Z","iopub.status.idle":"2024-01-08T13:21:38.391675Z","shell.execute_reply.started":"2024-01-08T13:21:38.324321Z","shell.execute_reply":"2024-01-08T13:21:38.390689Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"sentiment\n1.0    100000\n0.0     79769\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"The techniques for performing sentiment analysis can be broken down into simple rule-based techniques and supervised machine learning approaches. Rule-based techniques are easier to apply since they do not require annotated training data. Supervised learning approaches provide better results but include the additional effort of labeling the data. \n\n- Sentiment analysis using lexicon-based approaches\n- Sentiment analysis by building additional features from text data and applying a supervised machine learning algorithm\n- Sentiment analysis using transfer learning technique and pretrained language models like BERT","metadata":{}},{"cell_type":"code","source":"! pip install textacy","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:34:45.618239Z","iopub.execute_input":"2024-01-08T12:34:45.618527Z","iopub.status.idle":"2024-01-08T12:34:59.689554Z","shell.execute_reply.started":"2024-01-08T12:34:45.618496Z","shell.execute_reply":"2024-01-08T12:34:59.688470Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting textacy\n  Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: cachetools>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (4.2.4)\nRequirement already satisfied: catalogue~=2.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (2.0.10)\nRequirement already satisfied: cytoolz>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from textacy) (0.12.2)\nCollecting floret~=0.10.0 (from textacy)\n  Obtaining dependency information for floret~=0.10.0 from https://files.pythonhosted.org/packages/16/ee/388a5c76c9292f4bef85d7ef895005bb39a0899f8004e9daceb57b2bb0c9/floret-0.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading floret-0.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\nCollecting jellyfish>=0.8.0 (from textacy)\n  Obtaining dependency information for jellyfish>=0.8.0 from https://files.pythonhosted.org/packages/26/87/8d31224804af9dfa7b34657e083b67b24b322c41dd9464b52218c1a33890/jellyfish-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading jellyfish-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\nRequirement already satisfied: joblib>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (1.3.2)\nRequirement already satisfied: networkx>=2.7 in /opt/conda/lib/python3.10/site-packages (from textacy) (3.1)\nRequirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (1.24.3)\nCollecting pyphen>=0.10.0 (from textacy)\n  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (2.31.0)\nRequirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (1.11.4)\nRequirement already satisfied: scikit-learn>=1.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (1.2.2)\nRequirement already satisfied: spacy~=3.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (3.7.2)\nRequirement already satisfied: tqdm>=4.19.6 in /opt/conda/lib/python3.10/site-packages (from textacy) (4.66.1)\nRequirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from cytoolz>=0.10.1->textacy) (0.12.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.10.0->textacy) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.10.0->textacy) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.10.0->textacy) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.10.0->textacy) (2023.11.17)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0->textacy) (3.2.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (8.2.1)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (2.4.8)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (6.3.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (1.10.12)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (68.1.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy~=3.0->textacy) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (4.5.0)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy~=3.0->textacy) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy~=3.0->textacy) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy~=3.0->textacy) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy~=3.0->textacy) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy~=3.0->textacy) (2.1.3)\nDownloading floret-0.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jellyfish-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyphen, jellyfish, floret, textacy\nSuccessfully installed floret-0.10.5 jellyfish-1.0.3 pyphen-0.14.0 textacy-0.13.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from textacy import preprocessing as tprep\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport re\nimport spacy\nfrom tqdm.autonotebook import tqdm\n\ntqdm.pandas()\n\nprocess = tprep.make_pipeline(\n    tprep.replace.emails,\n    tprep.replace.emojis,\n    tprep.replace.urls,\n    tprep.replace.phone_numbers,\n    tprep.replace.hashtags,\n    tprep.replace.currency_symbols,\n    lambda text: re.sub(r\"\\n\", \" \", text),\n    tprep.remove.html_tags,\n    tprep.remove.brackets,\n    tprep.remove.punctuation,\n    tprep.normalize.hyphenated_words,\n    tprep.normalize.quotation_marks,\n    tprep.normalize.unicode,\n    tprep.normalize.bullet_points,\n    tprep.normalize.whitespace,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:34:59.691070Z","iopub.execute_input":"2024-01-08T12:34:59.691386Z","iopub.status.idle":"2024-01-08T12:35:06.652782Z","shell.execute_reply.started":"2024-01-08T12:34:59.691360Z","shell.execute_reply":"2024-01-08T12:35:06.651721Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"ds['clean_text'] = ds['text'].progress_apply(process)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:35:06.653886Z","iopub.execute_input":"2024-01-08T12:35:06.654440Z","iopub.status.idle":"2024-01-08T12:38:39.146517Z","shell.execute_reply.started":"2024-01-08T12:35:06.654412Z","shell.execute_reply":"2024-01-08T12:38:39.145696Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/179769 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5119bb57cc74a65a2eeac0890f0bee9"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Lexicon Based Approaches\n\nWhat is a lexicon? A lexicon is like a dictionary that contains a collection of words and has been compiled using expert knowledge. The key differentiating factor for a lexicon is that it incorporates specific knowledge and has been collected for a specific purpose. We will use sentiment lexicons that contain commonly used words and capture the sentiment associated with them. A simple example of this is the word happy, with a sentiment score of 1, and another is the word frustrated, which would have a score of -1. Several standardized lexicons are available for the English language, and the popular ones are AFINN Lexicon, SentiWordNet, Bing Liu’s lexicon, and VADER lexicon, among others. They differ from each other in the size of their vocabulary and their representation. For example, the AFINN Lexicon comes in the form of a single dictionary with 3,300 words, with each word assigned a signed sentiment score ranging from -3 to +3. Negative/positive indicate the polarity, and the magnitude indicates the strength. On the other hand, if we look at Bing Liu lexicon, it comes in the form of two lists: one for positive words and another for negative, with a combined\nvocabulary of 6,800 words.","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import opinion_lexicon\nfrom nltk.tokenize import word_tokenize\nimport random\n\n\nprint('Total number of words in opinion lexicon', len(opinion_lexicon.words()))\nprint('Examples of positive words in opinion lexicon', random.sample(sorted(opinion_lexicon.positive()), 5))\nprint('Examples of negative words in opinion lexicon', random.sample(sorted(opinion_lexicon.negative()), 5))","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:38:39.147846Z","iopub.execute_input":"2024-01-08T12:38:39.148539Z","iopub.status.idle":"2024-01-08T12:38:39.820427Z","shell.execute_reply.started":"2024-01-08T12:38:39.148501Z","shell.execute_reply":"2024-01-08T12:38:39.819516Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Total number of words in opinion lexicon 6789\nExamples of positive words in opinion lexicon ['affectation', 'proud', 'majestic', 'fervent', 'easy-to-use']\nExamples of negative words in opinion lexicon ['annihilation', 'inconstant', 'rebuke', 'delay', 'adversarial']\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\n\n\nnltk.download('punkt')\n\n\npos_score, neg_score = 1, -1\nword_dict = {}\n\n# Adding the positive words to the dictionary\nfor word in opinion_lexicon.positive():\n        word_dict[word] = pos_score\n        \n# Adding the negative words to the dictionary\nfor word in opinion_lexicon.negative():\n        word_dict[word] = neg_score\n        \n\ndef bing_liu_score(text):\n    sentiment_score = 0\n    bag_of_words = word_tokenize(text.lower())\n    \n    for word in bag_of_words:\n        if word in word_dict:\n            sentiment_score += word_dict[word]\n    \n    return sentiment_score / len(bag_of_words)\n\nds['Bing_Liu_Score'] = ds['text'].progress_apply(bing_liu_score)\nds.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:38:39.825260Z","iopub.execute_input":"2024-01-08T12:38:39.825873Z","iopub.status.idle":"2024-01-08T12:42:11.716125Z","shell.execute_reply.started":"2024-01-08T12:38:39.825841Z","shell.execute_reply":"2024-01-08T12:42:11.715189Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/179769 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2d9d2df342d48e5a31f76de5d585279"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"       id  product_id                                         summary  \\\n0  348179  B000O160KE                 Sweet & Low without the cancer.   \n1  306508  B004NB79VU                                     wedding mom   \n2  228313  B003VXHGPK  Don't waste your money or your Keurig on this!   \n3  448369  B0030FGMFY                                MADE IN CHINA!!!   \n4  515441  B004S04X4W                 Tastes like cheap meat and salt   \n\n                                                text  score  sentiment  \\\n0  If you like the (bitter) taste of Sweet & Low,...      1        0.0   \n1  item was much smaller than appeared on line.  ...      1        0.0   \n2  This coffee tastes very flavorful and is not t...      1        0.0   \n3  I bought these for my Dalmatian for the first ...      1        0.0   \n4  I guess I am in the minority, but this hash pr...      1        0.0   \n\n                                          clean_text  Bing_Liu_Score  \n0  If you like the taste of Sweet Low get this If...        0.020619  \n1  item was much smaller than appeared on line Yo...        0.000000  \n2  This coffee tastes very flavorful and is not t...        0.005917  \n3  I bought these for my Dalmatian for the first ...        0.000000  \n4  I guess I am in the minority but this hash pro...       -0.006135  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>product_id</th>\n      <th>summary</th>\n      <th>text</th>\n      <th>score</th>\n      <th>sentiment</th>\n      <th>clean_text</th>\n      <th>Bing_Liu_Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>348179</td>\n      <td>B000O160KE</td>\n      <td>Sweet &amp; Low without the cancer.</td>\n      <td>If you like the (bitter) taste of Sweet &amp; Low,...</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>If you like the taste of Sweet Low get this If...</td>\n      <td>0.020619</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>306508</td>\n      <td>B004NB79VU</td>\n      <td>wedding mom</td>\n      <td>item was much smaller than appeared on line.  ...</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>item was much smaller than appeared on line Yo...</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>228313</td>\n      <td>B003VXHGPK</td>\n      <td>Don't waste your money or your Keurig on this!</td>\n      <td>This coffee tastes very flavorful and is not t...</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>This coffee tastes very flavorful and is not t...</td>\n      <td>0.005917</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>448369</td>\n      <td>B0030FGMFY</td>\n      <td>MADE IN CHINA!!!</td>\n      <td>I bought these for my Dalmatian for the first ...</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>I bought these for my Dalmatian for the first ...</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>515441</td>\n      <td>B004S04X4W</td>\n      <td>Tastes like cheap meat and salt</td>\n      <td>I guess I am in the minority, but this hash pr...</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>I guess I am in the minority but this hash pro...</td>\n      <td>-0.006135</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Now that we have calculated the sentiment score, we would like to check whether the calculated score matches the expectation based on the rating provided by the customer. Instead of checking this for each review, we could compare the sentiment score across reviews that have different ratings. We would expect that a review that has a five-star rating would have a higher sentiment score than a review with a one star rating","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import scale\n\n\nds['Bing_Liu_Score'] = scale(ds['Bing_Liu_Score'])\nds.groupby('score').agg({'Bing_Liu_Score':'mean'})","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:42:11.717174Z","iopub.execute_input":"2024-01-08T12:42:11.717450Z","iopub.status.idle":"2024-01-08T12:42:11.740349Z","shell.execute_reply.started":"2024-01-08T12:42:11.717426Z","shell.execute_reply":"2024-01-08T12:42:11.739400Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"       Bing_Liu_Score\nscore                \n1           -0.658122\n2           -0.320433\n4            0.311744\n5            0.537158","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bing_Liu_Score</th>\n    </tr>\n    <tr>\n      <th>score</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>-0.658122</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.320433</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.311744</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.537158</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Disadvantages of a Lexicon-Based Approach\n\nWhile the lexicon-based approach is simple, it has some obvious disadvantages.\n\n- First, we are bound by the size of the lexicon; if a word does not exist in the chosen lexicon, then we are unable to use this information while determining the sentiment score for this review. In the ideal scenario, we would like to use a lexicon that captures all the words in the language, but this is not feasible.\n- Second, we assume that the chosen lexicon is a gold standard and trust the sentiment score/polarity provided by the author(s). This is a problem because a particular lexicon may not be the right fit for a given use case. The Bing Liu lexicon is relevant here because it captures the online usage of language and includes common typos and slang in its lexicon. But if we were working on a dataset of tweets, then the VADER lexicon would be better suited since it includes support for popular acronyms (e.g., LOL) and emojis.\n- Finally, one of the biggest disadvantages of lexicons is that they overlook negation. Since the lexicon only matches words and not phrases, this would result in a negative score for a sentence that contains not bad when it actually is more neutral.","metadata":{}},{"cell_type":"markdown","source":"## Applying Simple Machine Learning Algorithm ","metadata":{}},{"cell_type":"code","source":"# https://github.com/nltk/nltk/issues/3028\n! unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:42:11.741438Z","iopub.execute_input":"2024-01-08T12:42:11.741738Z","iopub.status.idle":"2024-01-08T12:42:13.111607Z","shell.execute_reply.started":"2024-01-08T12:42:11.741714Z","shell.execute_reply":"2024-01-08T12:42:13.110526Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.corpus import wordnet\n# from nltk.tokenize import word_tokenize\nfrom spacy.lang.en import STOP_WORDS\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import WhitespaceTokenizer\n\n\ndef get_wordnet_pos(pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n    \n\nlemmas = tprep.make_pipeline(\n    lambda t: t.lower(),\n    tprep.remove.punctuation,\n    tprep.replace.numbers,\n    WhitespaceTokenizer().tokenize,   # This returns a list so be careful with functions afterwards\n    lambda t: [x for x in t if x not in STOP_WORDS],\n    lambda t: [x for x in t if len(x) > 0],\n    pos_tag,\n    lambda t: [WordNetLemmatizer().lemmatize(x[0], get_wordnet_pos(x[1])) for x in t],\n    lambda t: [x for x in t if len(x) > 1],\n    lambda t: \" \".join(t)\n)\n\nds['lemmas'] = ds['clean_text'].progress_apply(lemmas)    ","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:42:13.113100Z","iopub.execute_input":"2024-01-08T12:42:13.113400Z","iopub.status.idle":"2024-01-08T12:52:42.445246Z","shell.execute_reply.started":"2024-01-08T12:42:13.113373Z","shell.execute_reply":"2024-01-08T12:52:42.444424Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/179769 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f212fb7d6342629aa4103c29be929f"}},"metadata":{}}]},{"cell_type":"code","source":"## Remove observations that are empty after the cleaning step\nds = ds[ds['lemmas'].str.len() != 0]","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:52:42.446448Z","iopub.execute_input":"2024-01-08T12:52:42.446749Z","iopub.status.idle":"2024-01-08T12:52:42.714445Z","shell.execute_reply.started":"2024-01-08T12:52:42.446722Z","shell.execute_reply":"2024-01-08T12:52:42.713658Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(ds['lemmas'], ds['sentiment'], test_size=0.2, random_state=42)\n\nprint ('Size of Training Data ', X_train.shape[0])\nprint ('Size of Test Data ', X_test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:52:42.715882Z","iopub.execute_input":"2024-01-08T12:52:42.716235Z","iopub.status.idle":"2024-01-08T12:52:42.744203Z","shell.execute_reply.started":"2024-01-08T12:52:42.716200Z","shell.execute_reply":"2024-01-08T12:52:42.743339Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Size of Training Data  143494\nSize of Test Data  35874\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\ntfidf = TfidfVectorizer(min_df=10, ngram_range=(1,1))\nX_train_tf = tfidf.fit_transform(X_train)\nX_test_tf = tfidf.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:52:42.745393Z","iopub.execute_input":"2024-01-08T12:52:42.745689Z","iopub.status.idle":"2024-01-08T12:52:50.230397Z","shell.execute_reply.started":"2024-01-08T12:52:42.745664Z","shell.execute_reply":"2024-01-08T12:52:50.229197Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\n\nmodel1 = LinearSVC(random_state=42, tol=1e-5)\nmodel1.fit(X_train_tf, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:52:50.231717Z","iopub.execute_input":"2024-01-08T12:52:50.232019Z","iopub.status.idle":"2024-01-08T12:52:52.382316Z","shell.execute_reply.started":"2024-01-08T12:52:50.231985Z","shell.execute_reply":"2024-01-08T12:52:52.381377Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"LinearSVC(random_state=42, tol=1e-05)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(random_state=42, tol=1e-05)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(random_state=42, tol=1e-05)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Baseline vs ML Model","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\n\ndef baseline_scorer(text):\n    return 1 if bing_liu_score(text) > 0 else 0\n    \ny_pred_baseline = X_test.progress_apply(baseline_scorer)\nprint(\"Baseline Accuracy Score: - \", accuracy_score(y_test, y_pred_baseline))\nprint(\"Baseline ROC-AUC Score: - \", roc_auc_score(y_test, y_pred_baseline))\n\ny_pred = model1.predict(X_test_tf)\nprint ('Accuracy Score - ', accuracy_score(y_test, y_pred))\nprint ('ROC-AUC Score - ', roc_auc_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-08T12:52:52.383410Z","iopub.execute_input":"2024-01-08T12:52:52.383708Z","iopub.status.idle":"2024-01-08T12:53:07.098130Z","shell.execute_reply.started":"2024-01-08T12:52:52.383683Z","shell.execute_reply":"2024-01-08T12:53:07.097267Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/35874 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bc388eedc4d441e9683eefcdc114aba"}},"metadata":{}},{"name":"stdout","text":"Baseline Accuracy Score: -  0.7161175224396499\nBaseline ROC-AUC Score: -  0.6999802330009797\nAccuracy Score -  0.8774599988849864\nROC-AUC Score -  0.8754400205839005\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We brought quite some improvements over the baseline here.","metadata":{}}]}